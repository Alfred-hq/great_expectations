from __future__ import annotations

from typing import TYPE_CHECKING, Any, Sequence, Union

from pydantic import BaseModel

from great_expectations.rule_based_profiler.domain_builder import (
    CategoricalColumnDomainBuilder,
)
from great_expectations.rule_based_profiler.helpers.cardinality_checker import (
    CardinalityChecker,
    CardinalityLimitMode,
)
from great_expectations.validator.metric_configuration import MetricConfiguration
from great_expectations.validator.metrics_calculator import MetricsCalculator

if TYPE_CHECKING:
    from typing_extensions import TypeAlias

    from great_expectations.datasource.fluent import BatchRequest, DataAsset
    from great_expectations.datasource.fluent.interfaces import Batch


class Metric(BaseModel):
    name: str
    placeholder_value: Any  # TODO: Add value model and replace this
    # TODO: Add fields, config


class Metrics(BaseModel):
    """Collection of Metric objects."""

    metrics: Sequence[Metric]
    # TODO: Add fields, config


CloudStorableTypes: TypeAlias = Union[Metrics,]  # TODO: are there better approaches?


class CloudStoreBackend:  # TODO: Name this better
    pass
    # TODO: Add methods
    # TODO: Investigate - If only implementing Create, can we use existing?

    def create(
        self, value_type: CloudStorableTypes, value: CloudStorableTypes
    ) -> None:  # TODO: How to annotate?
        print(
            f"Creating item of type {value_type.__name__} in CloudStoreBackend - sending a POST REST API request to the cloud."
        )
        print(value)


class ColumnDescriptiveMetricsStore:
    pass
    # TODO: Add methods

    def __init__(self, backend: CloudStoreBackend):
        self._backend = backend

    def create(self, metrics: Metrics) -> None:
        print("Creating metric in ColumnDescriptiveMetricsStore")
        self._backend.create(
            value_type=Metrics, value=metrics
        )  # TODO: How to annotate/implement?


# TODO: How does agent pass batch request? Assuming batch_request is necessary to specify a specific batch for introspection
#  rather than just the data asset as a full batch.
def inspect_asset(asset: DataAsset, batch_request: BatchRequest | None) -> Metrics:
    """Inspect a DataAsset and return Metrics."""
    print("This method would be invoked via an AgentAction")
    print(f"Inspecting data asset: {asset.name}")
    print(
        "NOTE: Here is where we can use things like DomainBuilder and MetricCalculator to calculate metrics"
    )
    if batch_request is None:
        batch_request = asset.build_batch_request()

    batch = _get_batch(asset, batch_request)

    # TODO: Use DomainBuilder and MetricCalculator to generate.
    metrics = _get_metrics_to_describe_batch(batch)
    return metrics


# TODO: Alternative approach - use an Inspector class:
# class AssetInspector:  # TODO: Name this better, or is this just a single method instead of class?
#     pass
#     # TODO: Add methods
#
#     def __init__(self, asset: DataAsset) -> None:
#         self._asset = asset
#
#     def inspect_batch(
#         self, batch_request: BatchRequest
#     ) -> Metrics:  # TODO: Should this take a batch instead of a batch request?
#         print("This method would be invoked via an AgentAction")
#         print(
#             f"Inspecting data asset: {self._asset.name} with batch request:\n{batch_request}"
#         )
#         print(
#             "NOTE: Here is where we can use things like DomainBuilder and MetricCalculator to calculate metrics"
#         )
#         metrics = Metrics(
#             metrics=[Metric(name="my_metric")]
#         )  # TODO: Use DomainBuilder and MetricCalculator to generate.
#         return metrics


def _get_batch(asset: DataAsset, batch_request: BatchRequest) -> Batch:
    batch_list = asset.get_batch_list_from_batch_request(batch_request=batch_request)
    if len(batch_list) > 1:
        # TODO: Better warning that we are only getting the first batch if there are multiple.
        print(
            "WARNING: More than one batch was returned. Only the first batch will be used."
        )
    batch = batch_list[0]
    # TODO: Error if no batches.
    return batch


def _get_domains(batch: Batch):
    pass
    # TODO: Implement
    # TODO: Do we need a rule as well?
    # domain_builder.get_domains()


def _convert_metrics_dict_to_metrics_object(raw_metrics: dict) -> Metrics:
    """Convert a dict of metrics to a Metrics object.

    Args:
        raw_metrics: Dict of metrics, where keys are metric names and values are metrics.
            Generated by the MetricsCalculator.

    Returns:
        Metrics object.
    """
    # TODO: Add the rest of the metric fields, convert value to Value object:
    metric_objs = [
        Metric(name=metric_name, placeholder_value=metric)
        for metric_name, metric in raw_metrics.items()
    ]
    return Metrics(metrics=metric_objs)


def _get_metrics_to_describe_batch(batch: Batch) -> Metrics:
    # TODO: Why do we typically get the execution engine from batch.data?
    #  Shouldn't it be from the datasource?
    metric_calculator = MetricsCalculator(execution_engine=batch.data.execution_engine)

    # TODO: Implement more than just table.head:
    # TODO: Better variable names for _to_get variables:
    table_summary_metrics_to_get = [
        MetricConfiguration(
            metric_name="table.head",
            metric_domain_kwargs={"batch_id": batch.id},
            metric_value_kwargs={
                "n_rows": 5,
                # "fetch_all": fetch_all,  # TODO: Is default already false?
            },
        ),
        MetricConfiguration(
            metric_name="table.row_count",
            metric_domain_kwargs={"batch_id": batch.id},
            metric_value_kwargs={},
        ),
    ]

    # TODO: Get domain_kwargs from DomainBuilder for columns.

    metrics_to_get = []
    metrics_to_get += table_summary_metrics_to_get
    metrics_to_get += _categorical_metrics_to_get(batch)

    # print("metric configs to get:", metrics_to_get)
    #
    # get_metrics_format = _convert_list_of_metric_configs_to_get_metrics_format(
    #     metric_configs=metrics_to_get
    # )

    return_val = []
    for metric in metrics_to_get:
        metric_to_get_in_get_metrics_format = (
            _convert_list_of_metric_configs_to_get_metrics_format([metric])
        )
        raw_metrics = metric_calculator.get_metrics(metric_to_get_in_get_metrics_format)
        metrics = _convert_metrics_dict_to_metrics_object(raw_metrics=raw_metrics)
        return_val.append(metrics)

    actual_return_val = []
    for metrics in return_val:
        actual_return_val += metrics.metrics
    return Metrics(metrics=actual_return_val)


def _convert_list_of_metric_configs_to_get_metrics_format(
    metric_configs: list[MetricConfiguration],
) -> dict:
    return_val = {}
    for metric_config in metric_configs:
        if not return_val.get(metric_config.metric_name):
            return_val[metric_config.metric_name] = [metric_config]
        else:
            return_val[metric_config.metric_name].append(metric_config)

    return return_val


def _categorical_metrics_to_get(batch: Batch) -> list[MetricConfiguration]:
    metric_calculator = MetricsCalculator(execution_engine=batch.data.execution_engine)
    print("getting columns")
    columns_raw_metric = metric_calculator.get_metrics(
        {
            metric_config.metric_name: metric_config
            for metric_config in [
                MetricConfiguration(
                    metric_name="table.columns",
                    metric_domain_kwargs={
                        "batch_id": batch.id,
                    },
                    metric_value_kwargs={
                        "include_nested": False,
                    },
                )
            ]
        }
    )
    columns_metric = _convert_metrics_dict_to_metrics_object(
        columns_raw_metric
    ).metrics[0]
    columns = list(columns_metric.placeholder_value)

    # TODO: Remove debug statements
    print("columns:", columns)

    print("Getting cardinality metrics")
    categorical_column_domain_builder = CategoricalColumnDomainBuilder(
        # VERY_FEW=AbsoluteCardinalityLimit("VERY_FEW", 10)
        cardinality_limit_mode=CardinalityLimitMode.VERY_FEW,
        max_unique_values=None,
        max_proportion_unique=None,
        data_context=None,
    )

    # TODO: This is a workaround to get _generate_metric_configurations_to_check_cardinality to work. It needs to be fixed.
    categorical_column_domain_builder._cardinality_checker = CardinalityChecker(
        cardinality_limit_mode=CardinalityLimitMode.VERY_FEW,  # TODO: Only needs to be set to get _generate_metric_configurations_to_check_cardinality to work.
    )
    # TODO: This is a private method - is there already a better way or an alternative or should we make this public?
    cardinality_metrics_to_get = categorical_column_domain_builder._generate_metric_configurations_to_check_cardinality(
        column_names=columns, batch_ids=[batch.id]
    )
    print("cardinality_metrics_to_get:", cardinality_metrics_to_get)

    # Flatten the dict of lists into a single list of values (MetricConfigurations):
    cardinality_metric_configs = [
        config for val in cardinality_metrics_to_get.values() for config in val
    ]

    # TODO: Does metric_config already have the column name? If so we might not need to add it here as a key.

    return cardinality_metric_configs


# Use ColumnDomainBuilder to get columns for numeric & datetime columns.
